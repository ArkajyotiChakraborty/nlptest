{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://159.20.93.138:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29550994b20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "spark = sparknlp.start()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "ner_dl download started this may take some time.\n",
      "Approximate size to download 13.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "\t\t.setInputCol(\"text\")\\\n",
    "\t\t.setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "\t\t.setInputCols([\"document\"])\\\n",
    "\t\t.setOutputCol(\"token\")\n",
    "\t\n",
    "embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
    "\t\t.setInputCols([\"document\", 'token']) \\\n",
    "\t\t.setOutputCol(\"embeddings\")\n",
    "\n",
    "ner = NerDLModel.pretrained(\"ner_dl\", 'en') \\\n",
    "\t\t.setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
    "\t\t.setOutputCol(\"ner\")\n",
    "\n",
    "ner_pipeline = Pipeline().setStages([\n",
    "\t\t\t\tdocumentAssembler,\n",
    "\t\t\t\ttokenizer,\n",
    "\t\t\t\tembeddings,\n",
    "\t\t\t\tner\n",
    "    ])\n",
    "ner_model = ner_pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': ['ner'],\n",
       " 'tests_types': ['uppercase', 'lowercase'],\n",
       " 'min_pass_rate': {'default': 0.5}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptest import Harness\n",
    "\n",
    "h3 = Harness(task=\"ner\", model=ner_model, hub=\"johnsnowlabs\", data=\"demo/data/test.conll\")\n",
    "h3.configure({\n",
    "    'tasks': ['ner'],\n",
    "    'tests_types': ['uppercase', 'lowercase'],\n",
    "    'min_pass_rate':{'default':0.5}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uppercase\n",
      "lowercase\n",
      "25 predictions have different lenghts than dataset and will be ignored.\n",
      "Please make sure dataset and model uses same tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_type</th>\n",
       "      <th>fail_count</th>\n",
       "      <th>pass_count</th>\n",
       "      <th>pass_rate</th>\n",
       "      <th>minimum_pass_rate</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uppercase</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>82%</td>\n",
       "      <td>50%</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lowercase</td>\n",
       "      <td>77.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15%</td>\n",
       "      <td>50%</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>micro-f1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>99%</td>\n",
       "      <td>50%</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macro-f1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>88%</td>\n",
       "      <td>50%</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_type fail_count pass_count pass_rate minimum_pass_rate   pass\n",
       "0  uppercase       16.0       75.0       82%               50%   True\n",
       "1  lowercase       77.0       14.0       15%               50%  False\n",
       "0   micro-f1          -          -       99%               50%   True\n",
       "1   macro-f1          -          -       88%               50%   True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3.generate().run().report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedGenderClassifier():\n",
    "    \"\"\"Rule based gender classifier for bias tests.\n",
    "    \"\"\"\n",
    "    def predict(self, text: str):\n",
    "        \"\"\"Predict the gender of given input sentence\n",
    "        \"\"\"\n",
    "        text = text.lower().split()\n",
    "        female_entities = ['she', 'her', 'hers', 'herself', 'girl', 'girls', 'woman', 'women',\n",
    "                        'madam', 'madame', 'lady', 'miss', 'mrs', 'female', 'breast', \n",
    "                        'ovary', 'ovarian', 'vagina']\n",
    "        male_entities = ['he', 'his', 'him', 'himself', 'boy', 'man', 'men', 'sir', 'gentleman', 'mr',\n",
    "                        'male', 'prostate', 'testicle', 'testicular', 'penis']\n",
    "        female_count = sum([text.count(ent) for ent in female_entities])\n",
    "        male_count = sum([text.count(ent) for ent in male_entities])\n",
    "\n",
    "        if female_count > male_count:\n",
    "            return \"F\"\n",
    "        elif male_count > female_count:\n",
    "            return \"M\"\n",
    "        else:\n",
    "            return \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gendered_data(data):\n",
    "    data = pd.Series(data)\n",
    "    sentences = pd.Series([x.original for x in data])\n",
    "    genders = sentences.apply(RuleBasedGenderClassifier().predict)\n",
    "    gendered_data = {\n",
    "        \"male\": data[genders == \"M\"].tolist(),\n",
    "        \"female\": data[genders == \"F\"].tolist(),\n",
    "        \"neutral\": data[genders == \"N\"].tolist(),\n",
    "    }\n",
    "    return gendered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9    30\n",
      "dtype: int64\n",
      "9    31\n",
      "dtype: int64\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "12    31\n",
      "31    31\n",
      "33    22\n",
      "35    25\n",
      "42     8\n",
      "dtype: int64\n",
      "12    32\n",
      "31    32\n",
      "33    23\n",
      "35    26\n",
      "42     9\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>test_type</th>\n",
       "      <th>actual_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>micro-f1</td>\n",
       "      <td>0.995050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>macro-f1</td>\n",
       "      <td>0.971522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>micro-f1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>macro-f1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>micro-f1</td>\n",
       "      <td>0.989831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>macro-f1</td>\n",
       "      <td>0.883046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_case test_type  actual_result\n",
       "0      male  micro-f1       0.995050\n",
       "1      male  macro-f1       0.971522\n",
       "2    female  micro-f1       1.000000\n",
       "3    female  macro-f1       1.000000\n",
       "4   neutral  micro-f1       0.989831\n",
       "5   neutral  macro-f1       0.883046"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptest.modelhandler.modelhandler import ModelFactory\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "gendered_data = get_gendered_data(h3.data)\n",
    "m = ModelFactory(ner_model,\"ner\")\n",
    "\n",
    "for key in gendered_data:\n",
    "    y_true = pd.Series(gendered_data[key]).apply(lambda x: [y.entity for y in x.expected_results.predictions])\n",
    "    X_test = pd.Series(gendered_data[key]).apply(lambda x: x.original)\n",
    "    y_pred = X_test.apply(m.predict_raw)\n",
    "    \n",
    "    valid_indices = y_true.apply(len) == y_pred.apply(len)\n",
    "    length_mismatch = valid_indices.count()-valid_indices.sum()\n",
    "    # if length_mismatch > 0:\n",
    "        # print(f\"{length_mismatch} predictions have different lenghts than dataset and will be ignored.\\nPlease make sure dataset and model uses same tokenizer.\")\n",
    "    y_true = y_true[valid_indices]\n",
    "    y_pred = y_pred[valid_indices]\n",
    "\n",
    "    y_true = y_true.explode().apply(lambda x: x.split(\"-\")[-1])\n",
    "    y_pred = y_pred.explode().apply(lambda x: x.split(\"-\")[-1])\n",
    "\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    gendered_data[key] = pd.DataFrame({\n",
    "        \"test_case\": [key, key],\n",
    "        \"test_type\": [\"micro-f1\", \"macro-f1\"],\n",
    "        \"actual_result\": [micro_f1_score, macro_f1_score]\n",
    "    })\n",
    "\n",
    "pd.concat(gendered_data.values(), ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c9408a94318707e8fdb8545d68f8a52ae12a13199d278674e95b123dca71b57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
